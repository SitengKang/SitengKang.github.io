---
title: "Poisoning Generative Replay in Continual Learning to Promote Forgetting"
collection: publications
permalink: /publication/2023
excerpt: 'We proposed a data poisoning attack on generative models in the context of replay based continual learning.'
date: 2023-6-15
venue: '40th International Conference on Machine Learning (ICML)'
paperurl: 'https://proceedings.mlr.press/v202/kang23c/kang23c.pdf'
citation: 'Kang, S., Shi, Z., & Zhang, X. (2023, July). Poisoning generative replay in continual learning to promote forgetting. In International Conference on Machine Learning (pp. 15769-15785). PMLR.'
---

Generative models have grown into the workhorse of many state-of-the-art machine learning methods. However, their vulnerability under poisoning attacks has been largely understudied. In this work, we investigate this issue in the context of continual learning, where generative replayers are utilized to tackle catastrophic forgetting. By developing a novel customization of dirty-label input-aware backdoors to the online setting, our attacker manages to stealthily promote forgetting while retaining high accuracy at the current task and sustaining strong defenders. Our approach taps into an intriguing property of generative models, namely that they cannot well capture input-dependent triggers. Experiments on four standard datasets corroborate the poisonerâ€™s effectiveness.
